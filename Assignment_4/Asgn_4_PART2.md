1.HOW MANY LAYERS:
------------------
In general we have only 3 layers. Input, hidden and output layer. As our objective is to detect the objects present in a given image, eventually we add more layers.
We add the layers procedurally, as the first layers to extract basic features like edges and gradients, the next layers for textures and patterns, 
next for parts of objects which collectively detects the objects. Adding layers would also improve the receptive field of an image. So we need to add as many layers 
required to reach the final(global) receptive field equal to the size of the image.   
5.RECEPTIVE FIELD:
------------------
Receptive field is the visibility of an image at the input layers when seen from an output pixel. The effective receptive field should always be equal to the 
size of image

2.MAXPOOLING:
-------------
To reduce an image of nxn to 1x1, we would use n/2 layers, which leads to a large number of parameters. So we need the pooling method to curb the parameter number. 
Max-pooling is one of such methods which progressively reduce the spatial size of the representation to reduce the amount of parameters and computation in the 
network by accumulating the features from maps generated by convolving a filter over an image. it reduces the computational cost by reducing the number of parameters 
to learn and provides basic translation, rotational and scaling invariance to the internal representation.
16.THE DISTANCE OF MAXPOOLING FROM PREDICTION:
----------------------------------------------
The Max-pooling is always done as far as possible from the final layers. Max-poling applied at the final layers would lead to some serious loss of 
information misleading the final prediction.
11.POSITION OF MAXPOOLING:
--------------------------
After applying enough kernel layers to fetch the class of features, we should perform max-pooling

3.1X1 CONVOLUTIONS:
-------------------
The number of filters used in the in the CNNs get increases with the depth in the network, which leads in the large number of features. As to extract these features,
it would require a lot of convolutional operations the network results in large number of parameters at a high computational cost. To address such problems we use 
pooling methods which down scale the image but do not reduce the channels or features. 1x1 has the single weight convolving around the input layer systematically 
with a stride of one, left-to-right and top-to-bottom, resulting in a feature map retaining the same size as the input. 1x1 is very simple which does not involve
any neighboring pixels which is considered as a linear projection of the input than a convolutional operation. Thus the 1x1 convolutions summarizes all the 
input features retaining the flexibility of nn depth.  

4.3X3 CONVOLUTIONS:
-------------------
First of all, we always prefers to use the odd-sized kernels for maintaining the symmetry across the output layer from the input layers, without which results in
major distortions in layers all over the network. The number of parameters increases with the kernel size, so as to limit this we either use 3x3 or 5x5.
But as of now we use 3x3 as the hardware manufacturers have optimized the hardware required for processing the model. Therefore, 3x3 kernels are optimal choice.  

6.SOFTMAX:
----------
Softmax often referred to as softargmax, takes n real numbers and converts them n real numbers which are summed up to result in 1, so as to interpret the values
as probabilities. This probability is directly proportional to the actual amplitude of the prediction. But as discussed in the class room, softmax is more a 
likelihood than a probability as we cannot apply probability on a perfectly quantified value.

7.LEARNING RATE:
----------------
Learning rate is a parameter or a positive values that determines rate of change of model in response to the estimated error each time the model weights are updated
Definitely choosing a learning rate is a daunting task. A very small learning rate requires many updates before hitting the local minima. A high learning rate would 
would cause drastic updates and leads to divergent behavior. only an optimum learning value would swiftly reach the minimum point.

8.KERNELS AND HOW DO WE DECIDE THE NUMBER OF KERNELS???
-------------------------------------------------------
A kernel is defined in a convolutional layer for extracting the information, compute and inform neuron about the feature. Kernels are convolved over images 
and extracts specific features. Kernels are more of like a 3x3 matrix which are also known as filters and feature extractors. 
The number of kernels to use, should be based on the complexity of datasets for a better training of the model. Intuitively, the number of kernels should be 
greater than the previous layers for possible number of combinations.
18.WHEN DO WE STOP CONVOLUTIONS AND GO AHEAD WITH A LARGER KERNEL OR SOME OTHER ALTERNATIVE???
----------------------------------------------------------------------------------------------


9.BATCH NORMALIZATION:
----------------------
In batch normalization we desaturate the kernel output values to find out the amplitude of the features extracted by the kernels irrespective of image properties, 
such as background contrast etc.
17.THE DISTANCE OF BATCH NORMALIZATION FROM PREDICTION:
Batch normalization is used after every layer but never before the last layer. 

10.IMAGE NORMALIZATION: (online definitions)
-----------------------
Image normalization is a process, often used in the preparation of data sets for artificial intelligence (AI), in which multiple images are put into a common 
statistical distribution in terms of size and pixel values. However, a single image can also be normalized within itself, 
by changing the range of pixel intensity values.

12.CONCEPT OF TRANSITION LAYERS: (online definitions)
A densely connected convolutional will have dense blocks. Since each dense block will increase the number of channels, adding too many of them will lead to an 
excessively complex model. A transition layer is used to control the complexity of the model. It reduces the number of channels by using the 1×1 convolutional 
layer and halves the height and width of the average pooling layer with a stride of 2, further reducing the complexity of the model.
13.POSITION OF TRANSITION LAYER: (could not answer)
--------------------------------

14.DROPOUT:
-----------
Drop-out is a regularization technique used to prevent overfitting. During training time, at each iteration, a neuron is temporarily “dropped” or 
disabled with probability p. This means all the inputs and outputs to this neuron will be disabled at the current iteration. The dropped-out neurons 
are re-sampled with probability p at every training step, so a dropped out neuron at one step can be active at the next one.
Dropout prevents the network to be too dependent on a small number of neurons, and forces every neuron to be able to operate independently
15.WHEN DO WE INTRODUCE DROPOUT, OR WHEN DO WE KNOW WE HAVE SOME OVERFITTING???
-------------------------------------------------------------------------------
Dropout is used at each and every layer, with very minimal values of p. we witness the overfitting when the Train Loss decreases, but the validation loss increases.\
This happens in a overly complex model with large number of parameters.


19.HOW DO WE KNOW OUR NETWORK IS NOT GOING WELL, COMPARATIVELY, VERY EARLY???
-----------------------------------------------------------------------------
If the training accuracy is not increasing with in the first 5 epochs, we can say that the network is not going so well.

20.BATCH SIZE, AND EFFECTS OF BATCH SIZE???
-------------------------------------------
Batch size is None of our concern, as we are poor;). Batch size controls the accuracy of the estimate of the error gradient when training neural networks.
Batch sizes are generally kept smaller as they are noisy, offering a regularizing effect and lower generalization error. Smaller batch sizes make it 
easier to fit one batch worth of training data in memory (i.e. when using a GPU)
