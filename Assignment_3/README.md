# Assignment 1

1. PROBLEM STATEMENT: \
	The Neural network is designed to train a model, which accepts an MNIST data image and \
	a random number represented in one-hot encoded format and \
	results in the output of predicted MNIST image and the addition of MNIST digit and the input random number. \
	
2. DATA REPRESENTATION: \
	Input 1 is an image(28x28) of a digit, which we procured from MNIST database.\
	Input 2 is a random positive integer (0-9), represented in one-hot encoded format. \
	
3. DATA GENERATION STRTEGY: \
	The data used for model is downloaded from MNIST database. This data further randomly split as \
	data for training and data for validation in the ratio of 8:2.\
	The other input is a random number generated by pre-defined Pytorch library functions.
	The random numbers are not generated with test and train set as they are limited no.of inputs, also eliminates the purpose of downloading. \
	
4. METHOD OF COMBINING TWO INPUTS: \
	Few convolution layers and linear transformation is applied on the first input image, which resulted in a 1x64 vector,\
	the other input is a one-hot encoded vector of dimension  1x10. This one-hot encoded vector and mnist embeddings are combined by concatenation. \

5. EVALUATION OF RESULTS:\
	From the data generated, 20% of the data is used for evaluation. During the training we fed a batch of data,\
	for several epochs to calculate the loss and accuracy for every epoch. \

6. PREDICTION ANALYSIS:\
	The output of model is a vector of size 1x29, in which we used the first 10 values to find out the predicted mnist image digit with argmax operation, \
	which is further compared with the ground truth that results in a binary vector. \
	The following 19 values are used to find out the summation result of both the inputs, by performing an argmax operation.\
	This result is further compared with ground truth to get a binary vector.\
	A logical AND operation is done on these two final binary vectors for making the final prediction.\

7. LOSS FUNCTION AND STRATEGY\
	Log-softmax is calculated on the predicted value and the loss is calculated using used negative log-likelihood strategy.
	In most of the classification problems we use Negative log-likelihood as a loss function. \
	It'll give high penalty for the wrong prediction estimations which helps to reach the global-minima swiftly.\



### Training Accuracy: 99.6%
### Validation Accuracy: 98.13%
### Testing Accuracy: 98.88%

## Model Architecture
![Model Architecture](./images/model.onnx.png)



Epoch:0	 TRAIN	 Correct Prediction:29952/48000 	 Loss:497.4713537991047\
Epoch:0	 VAL	 Correct Prediction:11509/12000 	 Loss:19.662540275603533\

====================\
Epoch:1	 TRAIN	 Correct Prediction:47032/48000 	 Loss:49.821423983201385\
Epoch:1	 VAL	 Correct Prediction:11701/12000 	 Loss:10.705956779420376\

====================\
Epoch:2	 TRAIN	 Correct Prediction:47357/48000 	 Loss:33.6641988302581\
Epoch:2	 VAL	 Correct Prediction:11680/12000 	 Loss:11.259712489321828\

====================\
Epoch:3	 TRAIN	 Correct Prediction:47457/48000 	 Loss:26.377806387026794\
Epoch:3	 VAL	 Correct Prediction:11725/12000 	 Loss:9.298309839563444\

====================\
Epoch:4	 TRAIN	 Correct Prediction:47544/48000 	 Loss:20.96459978679195\
Epoch:4	 VAL	 Correct Prediction:11560/12000 	 Loss:17.600837492384017\

====================\
Epoch:5	 TRAIN	 Correct Prediction:47593/48000 	 Loss:19.187772662960924\
Epoch:5	 VAL	 Correct Prediction:11693/12000 	 Loss:12.292001076042652\

====================\
Epoch:6	 TRAIN	 Correct Prediction:47650/48000 	 Loss:17.352192566613667\
Epoch:6	 VAL	 Correct Prediction:11752/12000 	 Loss:9.537399587919936\

====================\
Epoch:7	 TRAIN	 Correct Prediction:47645/48000 	 Loss:16.01146376138786\
Epoch:7	 VAL	 Correct Prediction:11764/12000 	 Loss:8.340133433579467\

====================\
Epoch:8	 TRAIN	 Correct Prediction:47699/48000 	 Loss:14.36512234370457\
Epoch:8	 VAL	 Correct Prediction:11754/12000 	 Loss:9.379177957773209\

====================\
Epoch:9	 TRAIN	 Correct Prediction:47662/48000 	 Loss:14.790546221716795\
Epoch:9	 VAL	 Correct Prediction:11748/12000 	 Loss:8.744419929105788\

====================\
Epoch:10	 TRAIN	 Correct Prediction:47756/48000 	 Loss:12.318187298253179\
Epoch:10	 VAL	 Correct Prediction:11751/12000 	 Loss:10.829392328858376\

====================\
Epoch:11	 TRAIN	 Correct Prediction:47684/48000 	 Loss:14.567827199760359\
Epoch:11	 VAL	 Correct Prediction:11734/12000 	 Loss:10.623305963119492\

====================\
Epoch:12	 TRAIN	 Correct Prediction:47737/48000 	 Loss:12.828054652229184\
Epoch:12	 VAL	 Correct Prediction:11750/12000 	 Loss:9.594421615824103\

====================\
Epoch:13	 TRAIN	 Correct Prediction:47795/48000 	 Loss:9.564015291776741\
Epoch:13	 VAL	 Correct Prediction:11764/12000 	 Loss:8.968471553409472\

====================\
Epoch:14	 TRAIN	 Correct Prediction:47762/48000 	 Loss:10.349676551079028\
Epoch:14	 VAL	 Correct Prediction:11754/12000 	 Loss:10.023655626457185\

====================\
Epoch:15	 TRAIN	 Correct Prediction:47836/48000 	 Loss:7.573681380265043\
Epoch:15	 VAL	 Correct Prediction:11760/12000 	 Loss:11.228677010338288\

====================\
Epoch:16	 TRAIN	 Correct Prediction:47796/48000 	 Loss:8.770743762943312\
Epoch:16	 VAL	 Correct Prediction:11770/12000 	 Loss:11.104613247327507\

====================\
Epoch:17	 TRAIN	 Correct Prediction:47838/48000 	 Loss:7.894126078135741\
Epoch:17	 VAL	 Correct Prediction:11774/12000 	 Loss:9.511844959575683\

====================\
Epoch:18	 TRAIN	 Correct Prediction:47805/48000 	 Loss:9.625479590144096\
Epoch:18	 VAL	 Correct Prediction:11751/12000 	 Loss:10.46750884805806\

====================\
Epoch:19	 TRAIN	 Correct Prediction:47808/48000 	 Loss:9.533772916081944\
Epoch:19	 VAL	 Correct Prediction:11776/12000 	 Loss:8.765461752424017\

====================



